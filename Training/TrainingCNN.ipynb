{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcdba5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15537081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb00bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading training and validation data and its labels\n",
    "train_data = pd.read_csv(parent_directory + '/Data/training_data.csv')\n",
    "val_data = pd.read_csv(parent_directory + '/Data/validation_data.csv')\n",
    "\n",
    "train_labels = train_data['label'].values\n",
    "val_labels = val_data['label'].values\n",
    "\n",
    "train_labels = np.array(train_labels,dtype='float64')\n",
    "val_labels = np.array(val_labels,dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9240250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the 784 images into (28,28,1) to train a CNN\n",
    "train_images = []\n",
    "val_images = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_images.append(train_data.iloc[i,:].values[1:].reshape(28,28,1))\n",
    "for i in range(len(val_data)):\n",
    "    val_images.append(val_data.iloc[i,:].values[1:].reshape(28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66298817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d5391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img(train_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling images so that the pixel values fall in between 0 and 1\n",
    "train_images = np.array(train_images,dtype='float64')/255.0\n",
    "val_images = np.array(val_images,dtype='float64')/255.0\n",
    "\n",
    "train_labels = train_labels.reshape(train_labels.shape[0],1)\n",
    "val_labels = val_labels.reshape(val_labels.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d1dae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape : (45000, 28, 28, 1)\n",
      "Training Data Shape : (45000, 1)\n",
      "Training Data Shape : (15000, 28, 28, 1)\n",
      "Training Data Shape : (15000, 1)\n"
     ]
    }
   ],
   "source": [
    "#printing train validation split data shapes\n",
    "print('Training Data Shape :', train_images.shape)\n",
    "print('Training Data Shape :', train_labels.shape)\n",
    "print('Training Data Shape :', val_images.shape)\n",
    "print('Training Data Shape :', val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4140b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ModelCNN, ModelResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abe5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ModelCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4cf2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    \"convolution1 filters\" : [8, 16, 32, 64],\n",
    "    \"convolution2 filters\" : [16, 32, 64, 128],\n",
    "    \"filter_size\" : [2,3],\n",
    "    \"dense_layer\" : [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5215d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 10:55:20.721698: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-11-14 10:55:20.721714: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-11-14 10:55:20.721718: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-11-14 10:55:20.721743: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-14 10:55:20.721755: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "cnn.grid_search_cnn(train_images, train_labels, val_images, val_labels, parameter_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78b523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28671e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eeb5356",
   "metadata": {},
   "source": [
    "# Grid Search For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "def grid_search_cnn(train_data, train_labels, val_data, val_labels, parameter_grid, epochs=1, batch_size=32):\n",
    "    best_accuracy = 0\n",
    "    best_parameters = None\n",
    "    all_results = []\n",
    "\n",
    "    for parameter_combination in product(*parameter_grid.values()):\n",
    "        # Create a dictionary of parameters for the current combination\n",
    "        current_parameters = dict(zip(parameter_grid.keys(), parameter_combination))\n",
    "        \n",
    "        filters_conv1 = current_parameters[\"convolution1 filters\"]\n",
    "        filters_conv2 = current_parameters[\"convolution2 filters\"]\n",
    "        filter_size = current_parameters[\"filter_size\"]\n",
    "        dense_layer_inp = current_parameters[\"dense_layer\"]\n",
    "        input_shape = (28, 28, 1)\n",
    "        # Create and compile the model using the current parameters\n",
    "        model = create_model(filters_conv1, filters_conv2, filter_size, dense_layer_inp, input_shape)\n",
    "\n",
    "        model.compile(optimizer='sgd',\n",
    "                      loss='SparseCategoricalCrossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_data, train_labels,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(val_data, val_labels),\n",
    "                            verbose=0)\n",
    "\n",
    "        # Evaluate the model on validation data\n",
    "        _, current_accuracy = model.evaluate(val_data, val_labels)\n",
    "\n",
    "        # Store the results\n",
    "        result = {'parameters': current_parameters, 'accuracy': current_accuracy, 'history': history}\n",
    "        all_results.append(result)\n",
    "\n",
    "        # Update the best parameters if the current model is better\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_parameters = current_parameters\n",
    "\n",
    "    return best_parameters, best_accuracy, all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12348b",
   "metadata": {},
   "source": [
    "# Training and tuning a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b26bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_model(filters_conv1, filters_conv2, filter_size, dense_layer_inp, input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters_conv1, (filter_size, filter_size),padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(filters_conv2, (filter_size, filter_size),padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    # model.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_layer_inp, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', \n",
    "                      loss='SparseCategoricalCrossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    \"convolution1 filters\" : [8, 16, 32, 64],\n",
    "    \"convolution2 filters\" : [16, 32, 64, 128],\n",
    "    \"filter_size\" : [2,3],\n",
    "    \"dense_layer\" : [32, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88619181",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_search_cnn(train_images, train_labels, val_images, val_labels, parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525889e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(32, 32, 3, 32, (28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e0969",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels,validation_data=(val_images, val_labels), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097078bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def mini_resnet(input_shape, num_classes, learning_rate=0.001, num_residual_blocks=3, num_filters=32):\n",
    "    input_tensor = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(num_filters, (3, 3), padding='same', activation='relu')(input_tensor)\n",
    "    \n",
    "    for _ in range(num_residual_blocks):\n",
    "        residual = x\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding='same', activation='relu')(x)\n",
    "        x = layers.Conv2D(num_filters, (3, 3), padding='same')(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    output_tensor = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (28, 28, 1)  \n",
    "num_classes = 10\n",
    "mini_resnet_model = mini_resnet(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07be5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_resnet_model.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_resnet_model.fit(train_images, train_labels,validation_data=(val_images, val_labels), epochs=12, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279da03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mini_resnet_model.fit(train_images, train_labels,validation_data=(val_images, val_labels), epochs=12, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9013967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = []\n",
    "for i in range(pred.shape[0]):\n",
    "    val_pred.append(np.argmax(pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "def evaluation_metrics(val_labels, val_pred):\n",
    "    # Create the confusion matrix\n",
    "    conf_matrix = confusion_matrix(val_labels, val_pred)\n",
    "\n",
    "    # Plot the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(val_labels, val_pred, average='weighted')\n",
    "    recall = recall_score(val_labels, val_pred, average='weighted')\n",
    "    f1 = f1_score(val_labels, val_pred, average='weighted')\n",
    "\n",
    "    # Print the precision, recall, and F1 score\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae476d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b9f3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluation_metrics(val_labels,val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437213c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# conv1 = [8, 16, 32, 64]\n",
    "# conv2 = [16, 32, 64, 128]\n",
    "# filter_size = [2,3]\n",
    "# dense_layer = [32, 64]\n",
    "\n",
    "# evaluation_dict = {}\n",
    "\n",
    "# for filters_conv1 in conv1:\n",
    "#     for filters_conv2 in conv2:\n",
    "#         for size in filter_size:\n",
    "#             for dense_layer_inp in dense_layer:\n",
    "#                 key = \"fc1 \" + str(filters_conv1) + \" fc2 \" + str(filters_conv2) + \" fs \" + str(filter_size) + \" dense inp \" + str(dense_layer_inp)\n",
    "#                 print('<---------------------------------------------------------------->')\n",
    "#                 model = fasion_model(filters_conv1, filters_conv2, size, dense_layer_inp, (28,28,1))\n",
    "#                 print('<-------------------------- Training ---------------------------->')\n",
    "#                 model.fit(train_images, train_labels,validation_data=(val_images, val_labels), epochs=3, batch_size=32,verbose=0)\n",
    "#                 print('<-------------------------- Testing ---------------------------->')\n",
    "#                 pred = model.predict(val_images,verbose=0)\n",
    "#                 val_pred = []\n",
    "#                 for i in range(pred.shape[0]):\n",
    "#                     val_pred.append(np.argmax(pred[i]))\n",
    "#                 precision = precision_score(val_labels, val_pred, average='weighted')\n",
    "#                 recall = recall_score(val_labels, val_pred, average='weighted')\n",
    "#                 score_dict = {}\n",
    "#                 f1 = f1_score(val_labels, val_pred, average='weighted')\n",
    "#                 score_dict['precision'] = precision\n",
    "#                 score_dict['recall'] = recall\n",
    "#                 score_dict['f1-score'] = f1\n",
    "#                 evaluation_dict[key] = score_dict\n",
    "#                 print(\"Model Parameters filters conv 1 \" + str(filters_conv1))\n",
    "#                 print(\"Model Parameters filters conv 2 \" + str(filters_conv2))\n",
    "#                 print(\"Model Parameters filter size\" + str(size))\n",
    "#                 print(\"Model Parameters dense layer inputs \" + str(dense_layer_inp))\n",
    "#                 print(\"Precision \" + f\"{precision:.5f}\" + \" Recall \" + f\"{recall:.5f}\" + \" f1 \" + f\"{f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, last_conv_layer_name, image, class_index):\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(image)\n",
    "        class_output = predictions[:, class_index]\n",
    "\n",
    "    grads = tape.gradient(class_output, conv_output)\n",
    "    pooled_grads = tf.reduce_mean(tf.nn.relu(grads), axis=(0, 1, 2))\n",
    "    conv_output = conv_output[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "img = val_images[2]\n",
    "class_index = np.argmax(model.predict(img.reshape(1,28,28,1)))\n",
    "\n",
    "\n",
    "last_conv_layer_name = 'conv2d_45'  \n",
    "heatmap = generate_grad_cam(model, last_conv_layer_name, img.reshape(1,28,28,1), class_index)\n",
    "\n",
    "#plot_img(img)\n",
    "\n",
    "# Display the Grad-CAM heatmap on top of the original image\n",
    "plt.imshow(heatmap)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = val_images[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffa06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have two lists: 'images' and 'heat_maps'\n",
    "# Each list contains the images and their corresponding heat maps\n",
    "\n",
    "# Create a figure and axis for the 2x5 grid\n",
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Loop through the images and heat maps and plot them in the grid\n",
    "for i in range(5):\n",
    "    # Plot the image in the first row\n",
    "    axs[0, i].imshow(images[i])\n",
    "    axs[0, i].set_title(\"Image\")\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "    # Plot the Grad-CAM heat map in the second row\n",
    "    axs[1, i].imshow(generate_grad_cam(model, last_conv_layer_name, images[i].reshape(1,28,28,1), class_index), cmap='jet', alpha=0.7)  # Adjust alpha for transparency\n",
    "    axs[1, i].set_title(\"Grad-CAM\")\n",
    "    axs[1, i].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Show the plot or save it to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fd5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'loss': ['binary_crossentropy', 'categorical_crossentropy'],\n",
    "    # Add other parameters as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "for parameter_combination in product(*parameter_grid.values()):\n",
    "    current_parameters = dict(zip(parameter_grid.keys(), parameter_combination))\n",
    "    print(current_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99fdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
